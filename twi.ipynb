{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c613f2",
   "metadata": {},
   "source": [
    "# Trying to bring all the different datasets together for analysis. \n",
    "Attempt to use weighed computations in xarray? Or resize/resample so that the soil grids data is the base size (250m x 250m). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947de32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import datetime \n",
    "import os\n",
    "import earthaccess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fc7ba",
   "metadata": {},
   "source": [
    "## Constants and shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dda54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "yesterday = today - datetime.timedelta(days=1)\n",
    "date = yesterday.strftime(\"%Y%m%d\")\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27cede",
   "metadata": {},
   "source": [
    "## Fetch Data\n",
    "Fetch Prism Data for yesterday. Note that PRISM day ends at 12:00 GMT, or 7:00 am Eastern Time. \n",
    "\n",
    "Prism servers their files as Cloud optimized GeoTiffs now, but they are given as zip files, so you can't stream them directly into rioxarray using open_rasterio(url). It needs to be downloaded, as I have done here. The zip file is loaded into memory, and then the .tif file is taken from memory and saved to disk as a temporary local file. Then the path is given to open_rasterio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ppt = f\"https://services.nacse.org/prism/data/get/us/800m/ppt/{date}\"\n",
    "url_tmean = f\"https://services.nacse.org/prism/data/get/us/800m/tmean/{date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e72136",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ppt = requests.get(url_ppt)\n",
    "response_ppt.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(io.BytesIO(response_ppt.content)) as z:\n",
    "    ppt_filename = [f for f in z.namelist() if f.endswith(\".tif\")][0]\n",
    "    with z.open(ppt_filename) as ppt_file:\n",
    "        with open(ppt_filename, \"wb\") as f:\n",
    "            f.write(ppt_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9770ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tmean = requests.get(url_tmean)\n",
    "response_tmean.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(io.BytesIO(response_tmean.content)) as z:\n",
    "    tmean_filename = [f for f in z.namelist() if f.endswith(\".tif\")][0]\n",
    "    with z.open(tmean_filename) as tmean_file:\n",
    "        with open(tmean_filename, \"wb\") as f:\n",
    "            f.write(tmean_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prism_ppt = rxr.open_rasterio(ppt_filename, masked=True)\n",
    "ds_prism_tmean = rxr.open_rasterio(tmean_filename, masked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try xr.align(dataset_1, dataset_2, join=\"exact\") to check if the grids are aligned correctly. I think it will throw and error if not. From Xarray in 45 min tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ffdb97",
   "metadata": {},
   "source": [
    "Remove the local file after it has been saved elsewhere. Could also look into python package tempfile to automatically handle this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(ppt_filename)\n",
    "os.remove(tmean_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81feb2aa",
   "metadata": {},
   "source": [
    "## SoilGrids\n",
    "\n",
    "Load Soil grids processed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_soil = xr.open_dataset(\"data/soils/soil_ds_5070.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece1ddd",
   "metadata": {},
   "source": [
    "## SMAP for baseline soil moisture\n",
    "\n",
    "Download and load into dataarray latest SMAP data for conUS. It looks like SPL4SMGP is actually a full coverage product, and updated every 3 hours. So even though the temporal latency is relatively high, I should be able to get fairly well updated data.  Earth Access seems to be promising but it is not fully documented yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a41260",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.today() \n",
    "start_date = today - datetime.timedelta(days=4)\n",
    "start_date = start_date.strftime(\"%Y-%m-%dT%H\")\n",
    "end_date = today.strftime(\"%Y-%m-%dT%H\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f1a62",
   "metadata": {},
   "source": [
    "sort_key=\"-end_date\" sorts them in descending order of end date. This allows me to get the latest. Since the data is 3-hourly, I need to provide the hour in my end_Date request too. This will make sure I get more than the 9pm-midnight UTC window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(strategy=\"netrc\")\n",
    "\n",
    "granules = earthaccess.search_data(\n",
    "    short_name='SPL4SMGP',\n",
    "    version='008',\n",
    "    daac='NSIDC',\n",
    "    provider='NSIDC_ECS',\n",
    "    doi='10.5067/T5RUATAQREF8',\n",
    "    bounding_box=(-126, 24, -65, 50),\n",
    "    temporal=(f\"{start_date}\", f\"{end_date}\"),\n",
    "    sort_key=\"-end_date\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f39315",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = earthaccess.get_fsspec_https_session()\n",
    "url = granules[0].data_links()[0] \n",
    "with fs.open(url, mode=\"rb\") as f:\n",
    "    ds_smap = xr.open_dataset(f, engine=\"h5netcdf\", group='Geophysical_Data')\n",
    "    ds_vars_smap = ds_smap[[\"sm_surface_wetness\", \"vegetation_greenness_fraction\", \"surface_temp\"]]\n",
    "    ds_vars_smap.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abca05",
   "metadata": {},
   "source": [
    "Need to use the .load() method to load the object into memory before leaving the with statement. Otherwise you get the error I/O on closed file. I've selected the variables I want before loading to manage the size of memory taken up. \n",
    "\n",
    "If need metadata or other info about the smap data, need to open the root level of the h5 file by removing group='Geophysical_Data'. The root level includes all of the attributes and metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97eb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vars_smap = ds_smap[[\"sm_surface_wetness\", \"vegetation_greenness_fraction\", \"surface_temp\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9df00",
   "metadata": {},
   "source": [
    "## Merge all the dataarrays together \n",
    "\n",
    "Get all of the dataarrays into one dataset with the same CRS and grid. I think the best option for my use case will be upsampling to the highest spatial resolution (250m x 250m of soilgrids) so that I can have the most cells overlapping with trail vectors + bufffer. Use rio.reproject_match(ds_soil) on the other ds to get them all into the same grid size. Then use xr.merge()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "can-i-ride-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
